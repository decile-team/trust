{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/wassal/trust-wassal/')\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from geomloss import SamplesLoss\n",
    "from trust.utils.models.resnet import ResNet18\n",
    "from trust.utils.models.resnet import ResNet50\n",
    "import random\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from trust.utils.custom_dataset_medmnist import load_biodataset_custom\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from trust.utils.utils import SubsetWithTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_cfg: {'per_class_train': {0: 100, 1: 5}, 'per_class_val': {0: 20, 1: 20}, 'per_class_lake': {0: 1050, 1: 53}, 'per_class_test': {0: 200, 1: 200}, 'sel_cls_idx': [1], 'per_imbclass_train': {0: 100, 1: 5}, 'per_imbclass_val': {0: 20, 1: 20}, 'per_imbclass_lake': {0: 1050, 1: 53}, 'per_imbclass_test': {0: 200, 1: 200}}\n",
      "Using downloaded and verified file: /home/wassal/trust-wassal/tutorials/All_Wassal/data/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /home/wassal/trust-wassal/tutorials/All_Wassal/data/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "[1] 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CIFAR10 dataset\n",
    "'''transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    #Get an initial set of 100 data points\n",
    "initial_indices = torch.randperm(len(trainset))[:100]\n",
    "initial_dataset = torch.utils.data.Subset(trainset, initial_indices)\n",
    "initial_dataloader = DataLoader(dataset=initial_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Randomly select 1000 unlabelled datapoints\n",
    "unlabelled_indices = torch.randperm(len(trainset))[:1000]\n",
    "unlabeled_dataset = torch.utils.data.Subset(trainset, unlabelled_indices)\n",
    "\n",
    "# Filter the dataset to include only two classes, say, classes 0 and 1\n",
    "labelled_indices = [i for i, (img, label) in enumerate(trainset) if label in [0, 1]]\n",
    "\n",
    "# From the filtered dataset, randomly select 100 datapoints\n",
    "labelled_indices = torch.randperm(len(labelled_indices))[:100]\n",
    "query_dataset = torch.utils.data.Subset(trainset, labelled_indices)\n",
    "'''\n",
    "\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#load the dataset in the class imbalance setting\n",
    "feature = \"classimb\"\n",
    "datadir = '/home/wassal/trust-wassal/tutorials/All_Wassal/data' #contains the npz file of the data_name dataset listed below\n",
    "dataset_name = 'pneumoniamnist'\n",
    "\n",
    "split_cfg = {\n",
    "             \"per_class_train\":{0:100,1:5}, \n",
    "             \"per_class_val\":{0:20,1:20}, \n",
    "             \"per_class_lake\":{0:1050,1:53},\n",
    "             \"per_class_test\":{0:200,1:200},\n",
    "             \"sel_cls_idx\":[1], \n",
    "             \"per_imbclass_train\":{0:100,1:5}, \n",
    "             \"per_imbclass_val\":{0:20,1:20}, \n",
    "             \"per_imbclass_lake\":{0:1050,1:53},\n",
    "             \"per_imbclass_test\":{0:200,1:200}}\n",
    "\n",
    "print(\"split_cfg:\",split_cfg)\n",
    "\n",
    "trainset, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_biodataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "    \n",
    "print(sel_cls_idx, num_cls)\n",
    "\n",
    "#create a dataset that merges trainset and val_set into a dataset of class target and non_target\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "class BinaryClassificationDataset(Dataset):\n",
    "    def __init__(self, original_dataset, sel_cls_idx):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.sel_cls_idx = sel_cls_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.original_dataset[idx]\n",
    "\n",
    "        # Convert the target into binary\n",
    "        if target == self.sel_cls_idx:\n",
    "            target = 0\n",
    "        else:\n",
    "            target = 1\n",
    "        \n",
    "        return data, target\n",
    "\n",
    "# Create binary datasets\n",
    "binary_trainset = BinaryClassificationDataset(trainset, sel_cls_idx[0])\n",
    "binary_val_set = BinaryClassificationDataset(val_set, sel_cls_idx[0])\n",
    "\n",
    "# Merge trainset and val_set\n",
    "targetset = ConcatDataset([binary_trainset, binary_val_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:\"+str(0) if torch.cuda.is_available() else \"cpu\"\n",
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'ResNet50':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet50(num_cls)\n",
    "        else:\n",
    "            model = models.resnet50()\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def displayTable(val_err_log, tst_err_log):\n",
    "    col1 = [str(i) for i in range(10)]\n",
    "    val_acc = [str(100-i) for i in val_err_log]\n",
    "    tst_acc = [str(100-i) for i in tst_err_log]\n",
    "    table = [col1, val_acc, tst_acc]\n",
    "    table = map(list, zip(*table))\n",
    "    print(tabulate(table, headers=['Class', 'Val Accuracy', 'Test Accuracy'], tablefmt='orgtbl'))\n",
    "\n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "#         print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    displayTable(val_err_log, tst_err_log)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.astype(np.float))[subset])\n",
    "    else:\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "#     print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    aug_trainloader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True, pin_memory=True)\n",
    "    return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set,imb_cls_idx):\n",
    "    \n",
    "    miscls_idx = []\n",
    "    \n",
    "    \n",
    "    for i in imb_cls_idx:\n",
    "        imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        miscls_idx += imb_cls_samples\n",
    "    print(\"Total samples from imbalanced classes as targets (Size of query set): \", len(miscls_idx))\n",
    "    return SubsetWithTargets(val_set, miscls_idx, val_set.targets[miscls_idx])\n",
    "\n",
    "def getPrivateSet(val_set,imb_cls_idx):\n",
    "    \n",
    "    miscls_idx = []\n",
    "    \n",
    "    \n",
    "    for i in imb_cls_idx:\n",
    "        imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        miscls_idx += imb_cls_samples\n",
    "    print(\"Total samples from imbalanced classes as targets (Size of query set): \", len(miscls_idx))\n",
    "    return SubsetWithTargets(val_set, miscls_idx, val_set.targets[miscls_idx])\n",
    "\n",
    "\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    if str(type(lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        subset_cls = torch.Tensor(lake_set.targets.astype(np.float))[subset]\n",
    "    else:\n",
    "        subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "def print_final_results(res_dict, sel_cls_idx):\n",
    "    print(\"Gain in overall test accuracy: \", res_dict['test_acc'][1]-res_dict['test_acc'][0])\n",
    "    bf_sel_cls_acc = np.array(res_dict['all_class_acc'][0])[sel_cls_idx]\n",
    "    af_sel_cls_acc = np.array(res_dict['all_class_acc'][1])[sel_cls_idx]\n",
    "    print(\"Gain in targeted test accuracy: \", np.mean(af_sel_cls_acc-bf_sel_cls_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, ind):\n",
    "        self.ind = ind\n",
    "        return\n",
    "    def __iter__(self):\n",
    "        return iter(self.ind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def feature_extraction(inp, layer_name,model):\n",
    "        feature = {}\n",
    "        \n",
    "        def get_features(name):\n",
    "            def hook(model, inp, output):\n",
    "                feature[name] = output.detach()\n",
    "            return hook\n",
    "        for name, layer in model._modules.items():\n",
    "            if name == layer_name:\n",
    "                layer.register_forward_hook(get_features(layer_name))\n",
    "        output = model(inp)\n",
    "        return torch.squeeze(feature[layer_name])\n",
    "\n",
    "def get_feature_embedding(dataset, unlabeled, layer_name, model):\n",
    "        dataloader = DataLoader(dataset, batch_size = 1000, shuffle = False)\n",
    "        features = []\n",
    "        if(unlabeled):\n",
    "            for batch_idx, inputs in enumerate(dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                batch_features = feature_extraction(inputs, layer_name,model)\n",
    "                features.append(batch_features)\n",
    "        else:\n",
    "            for batch_idx, (inputs,_) in enumerate(dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                batch_features = feature_extraction(inputs, layer_name,model)\n",
    "                features.append(batch_features)\n",
    "        return torch.vstack(features)\n",
    "\n",
    "\n",
    "\n",
    "def _proj_simplex(v):\n",
    "    \"\"\"\n",
    "    v: PyTorch Tensor to be projected to a simplex\n",
    "\n",
    "    Returns:\n",
    "    w: PyTorch Tensor simplex projection of v\n",
    "    \"\"\"\n",
    "    z = 1\n",
    "    orig_shape = v.shape\n",
    "    v = v.view(1, -1)\n",
    "    shape = v.shape\n",
    "    with torch.no_grad():\n",
    "        mu = torch.sort(v, dim=1)[0]\n",
    "        mu = torch.flip(mu, dims=(1,))\n",
    "        cum_sum = torch.cumsum(mu, dim=1)\n",
    "        j = torch.unsqueeze(torch.arange(1, shape[1] + 1, dtype=mu.dtype, device=mu.device), 0)\n",
    "        rho = torch.sum(mu * j - cum_sum + z > 0.0, dim=1, keepdim=True) - 1.\n",
    "        rho = rho.to(int)\n",
    "        max_nn = cum_sum[torch.arange(shape[0]), rho[:, 0]]\n",
    "        theta = (torch.unsqueeze(max_nn, -1) - z) / (rho.type(max_nn.dtype) + 1)\n",
    "        w = torch.clamp(v - theta, min=0.0).view(orig_shape)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targetset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Step : Fine-tune the ResNet on the initial data until overfitting\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#Create dataloaders\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(targetset, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                               shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mresnet50(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targetset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step : Fine-tune the ResNet on the initial data until overfitting\n",
    "#Create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(targetset, batch_size=100,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model=model.to(device)\n",
    "\n",
    "args = {}\n",
    "model_name = 'ResNet18'\n",
    "embedding_type = \"features\" #Type of the representation to use (gradients/features)\n",
    "args['classifier_learning_rate'] = 0.0003\n",
    "# Model Creation\n",
    "#model = create_model(model_name, num_cls, device, embedding_type)\n",
    "# Loss Functions\n",
    "criterion, criterion_nored = loss_function()\n",
    "\n",
    "# Getting the optimizer and scheduler\n",
    "learning_rate = args['classifier_learning_rate']\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0.0\n",
    "full_trn_loss=0.0\n",
    "full_trn_correct = 0\n",
    "full_trn_total = 0\n",
    "num_ep=1\n",
    "while(full_trn_acc<0.99 and num_ep<100):\n",
    "            model.train()\n",
    "            # if strategy != \"WASSAL_Pure_Weighted\":\n",
    "            # if True:\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                #inputs, target = Variable(inputs), Variable(inputs)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                # print(\"hard loss:{}\".format(loss.item()))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader): #Compute Train accuracy\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc = full_trn_correct / full_trn_total\n",
    "                print(\" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc, end=\"\\r\")\n",
    "                num_ep+=1\n",
    "\n",
    "loss_func = SamplesLoss(\"sinkhorn\", p=2, blur=0.1, scaling=0.1)\n",
    "\n",
    "unlabeled_dataset_len=len(lake_set)\n",
    "shuffled_indices = list(range(unlabeled_dataset_len))\n",
    "random.shuffle(shuffled_indices)\n",
    "sampler = customSampler(shuffled_indices)\n",
    "\n",
    "query_dataset = getQuerySet(val_set,split_cfg['sel_cls_idx'])\n",
    "query_dataset_len = len(query_dataset)\n",
    "\n",
    "refrain_dataset = getPrivateSet(val_set,split_cfg['sel_cls_idx'])\n",
    "refrain_dataset_len = len(refrain_dataset)\n",
    "\n",
    "minibatch_size = 500\n",
    "step_size=10\n",
    "num_batches = math.ceil(unlabeled_dataset_len/minibatch_size)\n",
    "\n",
    "\n",
    " #uniform distribution of weights\n",
    "simplex_query= Variable(torch.ones(unlabeled_dataset_len, requires_grad=True, device=device)/unlabeled_dataset_len)\n",
    "simplex_refrain = Variable(torch.ones(unlabeled_dataset_len, requires_grad=True, device=device)/unlabeled_dataset_len) \n",
    "beta = torch.ones(query_dataset_len, requires_grad=False)/query_dataset_len\n",
    "gamma = torch.ones(refrain_dataset_len, requires_grad=False)/refrain_dataset_len\n",
    "unlabeled_dataloader = DataLoader(dataset=lake_set, batch_size=minibatch_size, shuffle=False, sampler=sampler)\n",
    "query_dataloader = DataLoader(dataset=query_dataset, batch_size=len(query_dataset), shuffle=False)\n",
    "refrain_dataloader = DataLoader(dataset=refrain_dataset, batch_size=len(refrain_dataset), shuffle=False)\n",
    "\n",
    "query_iter=iter(query_dataloader)\n",
    "refrain_iter=iter(refrain_dataloader)\n",
    "\n",
    "query_imgs, _= next(query_iter)\n",
    "query_imgs=query_imgs.to(device)\n",
    "refrain_imgs, _= next(refrain_iter)\n",
    "refrain_imgs=refrain_imgs.to(device)\n",
    "\n",
    "beta = beta.to(device)\n",
    "gamma = gamma.to(device)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.1\n",
    "step_size = 5\n",
    "optimizer = torch.optim.Adam([simplex_query,beta], lr=lr)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler_query = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "simplex_query.requires_grad = True\n",
    "#beta.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "# Create lists to store the loss values\n",
    "        \n",
    "overall_loss=[]\n",
    "        \n",
    "# Loop over the datasets 10 times\n",
    "for i in range(100):\n",
    "    \n",
    "    simplex_query.grad = None  # Reset gradients at the beginning of each epoch\n",
    "    batch_idx = 0\n",
    "    # Initialize loss_avg as a tensor with requires_grad=True\n",
    "    loss_avg = torch.tensor(0.0, requires_grad=True)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    #batchwise WD calculation\n",
    "    for unlabeled_imgs,_ in unlabeled_dataloader:\n",
    "         # Don't calculate gradients\n",
    "        with torch.no_grad():     \n",
    "            unlabeled_features = get_feature_embedding(unlabeled_imgs, True, 'avgpool',model)\n",
    "            query_features = get_feature_embedding(query_imgs, True, 'avgpool',model)\n",
    "            refrain_features = get_feature_embedding(refrain_imgs, True, 'avgpool',model)\n",
    "                    \n",
    "        # Flatten the features to be 1D tensors\n",
    "        unlabeled_features = unlabeled_features.view(unlabeled_features.shape[0], -1)\n",
    "        query_features = query_features.view(query_features.shape[0], -1) \n",
    "        refrain_features = refrain_features.view(refrain_features.shape[0], -1)\n",
    "\n",
    "        simplex_batch_query = simplex_query[batch_idx * unlabeled_dataloader.batch_size : (batch_idx + 1) * unlabeled_dataloader.batch_size]\n",
    "        \n",
    "        #TODO: should we average or project?\n",
    "        simplex_batch_query = simplex_batch_query.clone() / simplex_batch_query.sum()\n",
    "        simplex_batch_query=simplex_batch_query.to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        simplex_batch_refrain = simplex_refrain[batch_idx * unlabeled_dataloader.batch_size : (batch_idx + 1) * unlabeled_dataloader.batch_size]\n",
    "        simplex_batch_refrain = simplex_batch_refrain.clone() / simplex_batch_refrain.sum()\n",
    "        simplex_batch_refrain=simplex_batch_refrain.to(device)\n",
    "\n",
    "        \n",
    "       \n",
    "       \n",
    "        loss = loss_func(simplex_batch_query, unlabeled_features, beta, query_features)\n",
    "\n",
    "        overall_loss.append(loss.item())\n",
    "                \n",
    "        loss_avg = loss_avg + loss / num_batches\n",
    "        #loss_avg=0.5*loss_avg\n",
    "        batch_idx += 1\n",
    "        print(\"Batchwise loss: {}\".format(loss), end=\"\\r\")\n",
    "            \n",
    "    loss_avg.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    scheduler_query.step()\n",
    "   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        simplex_query.data = _proj_simplex(simplex_query.data)\n",
    "   \n",
    "    print(\"Epoch:[]\", i, \"]. Avg loss: [{}]\".format(loss_avg), end=\"\\r\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming your unlabeled_dataset is a PyTorch Dataset that returns a tuple of (image, class) \n",
    "    # and that 'classes' is a list of class labels for each image in the dataset\n",
    "\n",
    "    classes = np.array([label for _, label in lake_set])\n",
    "\n",
    "    # Converting the simplex_target tensor to numpy array for plotting\n",
    "    simplex_np = simplex_query.detach().cpu().numpy()\n",
    "\n",
    "    # Setting up the y-values to be 1 since it's a one-dimensional plot\n",
    "    y = classes\n",
    "\n",
    "    # create a color map to color your data points based on their class\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, 2))  # 10 distinct colors for 10 classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(0, 2):  # assuming classes range from 1 to 10\n",
    "        ax.scatter(simplex_np[classes==i], y[classes==i], color=colors[i-1], label='Class '+str(i))\n",
    "\n",
    "    ax.set_yticks([])  # remove the y-axis\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targetset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Step : Fine-tune the ResNet on the initial data until overfitting\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#Create dataloaders\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(targetset, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                               shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39m#model = models.resnet50(pretrained=True)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#model=model.to(device)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m args \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targetset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step : Fine-tune the ResNet on the initial data until overfitting\n",
    "#Create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(targetset, batch_size=100,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#model=model.to(device)\n",
    "\n",
    "args = {}\n",
    "model_name = 'ResNet18'\n",
    "embedding_type = \"features\" #Type of the representation to use (gradients/features)\n",
    "args['classifier_learning_rate'] = 0.0003\n",
    "# Model Creation\n",
    "model = create_model(model_name, num_cls, device, embedding_type)\n",
    "# Loss Functions\n",
    "criterion, criterion_nored = loss_function()\n",
    "\n",
    "# Getting the optimizer and scheduler\n",
    "learning_rate = args['classifier_learning_rate']\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0.0\n",
    "full_trn_loss=0.0\n",
    "full_trn_correct = 0\n",
    "full_trn_total = 0\n",
    "num_ep=1\n",
    "while(full_trn_acc<0.99 and num_ep<100):\n",
    "            model.train()\n",
    "            # if strategy != \"WASSAL_Pure_Weighted\":\n",
    "            # if True:\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                # print(\"hard loss:{}\".format(loss.item()))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader): #Compute Train accuracy\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc = full_trn_correct / full_trn_total\n",
    "                print(\" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc, end=\"\\r\")\n",
    "                num_ep+=1\n",
    "\n",
    "loss_func = SamplesLoss(\"sinkhorn\", p=2, blur=0.1, scaling=0.1)\n",
    "\n",
    "unlabeled_dataset_len=len(lake_set)\n",
    "shuffled_indices = list(range(unlabeled_dataset_len))\n",
    "random.shuffle(shuffled_indices)\n",
    "sampler = customSampler(shuffled_indices)\n",
    "\n",
    "query_dataset = getQuerySet(trainset,split_cfg['sel_cls_idx'])\n",
    "query_dataset_len = len(query_dataset)\n",
    "\n",
    "private_dataset = getPrivateSet(trainset,split_cfg['sel_cls_idx'])\n",
    "private_dataset_len = len(private_dataset)\n",
    "\n",
    "minibatch_size = 500\n",
    "step_size=10\n",
    "num_batches = math.ceil(unlabeled_dataset_len/minibatch_size)\n",
    "\n",
    "\n",
    " #uniform distribution of weights\n",
    "simplex_target= Variable(torch.ones(unlabeled_dataset_len, requires_grad=True, device=device)/unlabeled_dataset_len)\n",
    "simplex_refrain = Variable(torch.ones(unlabeled_dataset_len, requires_grad=True, device=device)/unlabeled_dataset_len) \n",
    "beta = torch.ones(query_dataset_len, requires_grad=False)/query_dataset_len\n",
    "gamma = torch.ones(private_dataset_len, requires_grad=False)/private_dataset_len\n",
    "unlabeled_dataloader = DataLoader(dataset=lake_set, batch_size=minibatch_size, shuffle=False, sampler=sampler)\n",
    "target_dataloader = DataLoader(dataset=query_dataset, batch_size=len(query_dataset), shuffle=False)\n",
    "private_dataloader = DataLoader(dataset=private_dataset, batch_size=len(private_dataset), shuffle=False)\n",
    "\n",
    "target_iter=iter(target_dataloader)\n",
    "private_iter=iter(private_dataloader)\n",
    "\n",
    "target_imgs, _= next(target_iter)\n",
    "target_imgs=target_imgs.to(device)\n",
    "private_imgs, _= next(private_iter)\n",
    "private_imgs=private_imgs.to(device)\n",
    "\n",
    "beta = beta.to(device)\n",
    "gamma = gamma.to(device)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "step_size = 40\n",
    "optimizer = torch.optim.Adam([simplex_target,beta], lr=lr)\n",
    "optimizer_refrain = torch.optim.Adam([simplex_refrain,gamma], lr=lr)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler_target = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "scheduler_refrain = torch.optim.lr_scheduler.StepLR(optimizer_refrain, step_size=step_size, gamma=0.1)\n",
    "simplex_target.requires_grad = True\n",
    "simplex_refrain.requires_grad = True  \n",
    "\n",
    "\n",
    "# Create lists to store the loss values\n",
    "        \n",
    "overall_loss=[]\n",
    "        \n",
    "# Loop over the datasets 10 times\n",
    "for i in range(100):\n",
    "    \n",
    "    simplex_target.grad = None  # Reset gradients at the beginning of each epoch\n",
    "    batch_idx = 0\n",
    "    # Initialize loss_avg as a tensor with requires_grad=True\n",
    "    loss_avg = torch.tensor(0.0, requires_grad=True)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    #batchwise WD calculation\n",
    "    for unlabeled_imgs,_ in unlabeled_dataloader:\n",
    "         # Don't calculate gradients\n",
    "        with torch.no_grad():     \n",
    "            unlabeled_features = get_feature_embedding(unlabeled_imgs, True, 'avgpool',model)\n",
    "            target_features = get_feature_embedding(target_imgs, True, 'avgpool',model)\n",
    "            private_features = get_feature_embedding(private_imgs, True, 'avgpool',model)\n",
    "                    \n",
    "        # Flatten the features to be 1D tensors\n",
    "        unlabeled_features = unlabeled_features.view(unlabeled_features.shape[0], -1)\n",
    "        target_features = target_features.view(target_features.shape[0], -1) \n",
    "        private_features = private_features.view(private_features.shape[0], -1)\n",
    "\n",
    "        simplex_batch_target = simplex_target[batch_idx * unlabeled_dataloader.batch_size : (batch_idx + 1) * unlabeled_dataloader.batch_size]\n",
    "        \n",
    "        #TODO: should we average or project?\n",
    "        simplex_batch_target = simplex_batch_target.clone() / simplex_batch_target.sum()\n",
    "        simplex_batch_target=simplex_batch_target.to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        simplex_batch_refrain = simplex_refrain[batch_idx * unlabeled_dataloader.batch_size : (batch_idx + 1) * unlabeled_dataloader.batch_size]\n",
    "        simplex_batch_refrain = simplex_batch_refrain.clone() / simplex_batch_refrain.sum()\n",
    "        simplex_batch_refrain=simplex_batch_refrain.to(device)\n",
    "\n",
    "        \n",
    "        l1 = loss_func(simplex_batch_target, unlabeled_features, beta, target_features)\n",
    "        l2 = loss_func(simplex_batch_target, unlabeled_features, gamma, private_features)\n",
    "        l3 = loss_func(simplex_batch_target, unlabeled_features, simplex_batch_refrain, unlabeled_features)\n",
    "        loss = (1*l1) + (1*l2) - (0.3*l3)\n",
    "\n",
    "        overall_loss.append(loss.item())\n",
    "                \n",
    "        loss_avg = loss_avg + loss / num_batches\n",
    "        #loss_avg=0.5*loss_avg\n",
    "        batch_idx += 1\n",
    "        print(\"Batchwise loss: {}\".format(loss), end=\"\\r\")\n",
    "            \n",
    "    loss_avg.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_refrain.step()\n",
    "    scheduler_target.step()\n",
    "    scheduler_refrain.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        simplex_target.data = _proj_simplex(simplex_target.data)\n",
    "   \n",
    "    print(\"Epoch:[]\", i, \"]. Avg loss: [{}]\".format(loss_avg), end=\"\\r\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming your unlabeled_dataset is a PyTorch Dataset that returns a tuple of (image, class) \n",
    "    # and that 'classes' is a list of class labels for each image in the dataset\n",
    "\n",
    "    classes = np.array([label for _, label in lake_set])\n",
    "\n",
    "    # Converting the simplex_target tensor to numpy array for plotting\n",
    "    simplex_np = simplex_target.detach().cpu().numpy()\n",
    "\n",
    "    # Setting up the y-values to be 1 since it's a one-dimensional plot\n",
    "    y = np.ones_like(simplex_np)\n",
    "\n",
    "    # create a color map to color your data points based on their class\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, 2))  # 10 distinct colors for 10 classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(0, 2):  # assuming classes range from 1 to 10\n",
    "        ax.scatter(simplex_np[classes==i], y[classes==i], color=colors[i-1], label='Class '+str(i))\n",
    "\n",
    "    ax.set_yticks([])  # remove the y-axis\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simplex_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your unlabeled_dataset is a PyTorch Dataset that returns a tuple of (image, class) \n",
    "# and that 'classes' is a list of class labels for each image in the dataset\n",
    "\n",
    "classes = np.array([label for _, label in lake_set])\n",
    "\n",
    "# Converting the simplex_target tensor to numpy array for plotting\n",
    "simplex_np = simplex_refrain.detach().cpu().numpy()\n",
    "\n",
    "# Setting up the y-values to be 1 since it's a one-dimensional plot\n",
    "y = classes\n",
    "\n",
    "# create a color map to color your data points based on their class\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 2))  # 10 distinct colors for 10 classes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 2):  # assuming classes range from 1 to 10\n",
    "    ax.scatter(simplex_np[classes==i], y[classes==i], color=colors[i-1], label='Class '+str(i))\n",
    "\n",
    "ax.set_yticks([])  # remove the y-axis\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have a way to get the class labels for your unlabeled_dataset\n",
    "classes = np.array([label for _, label in unlabeled_dataset])  # replace with your method of getting labels\n",
    "\n",
    "# Converting the simplex_target tensor to numpy array for plotting\n",
    "simplex_np = simplex_target.detach().cpu().numpy()\n",
    "\n",
    "# create a color map to color your data points based on their class\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 10))  # 10 distinct colors for 10 classes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(1, 11):  # assuming classes range from 1 to 10\n",
    "    indices = np.where(classes == i)\n",
    "    sns.distplot(simplex_np[indices], color=colors[i-1], label='Class '+str(i), ax=ax)\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".wassalvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
