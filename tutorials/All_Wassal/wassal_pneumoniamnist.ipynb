{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087af8f6",
   "metadata": {
    "id": "Y4YlT-8B8lLN"
   },
   "source": [
    "# Targeted Selection Demo For Biomedical Datasets With Rare Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fc69c",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b905244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMIAA-Ua8lLR",
    "outputId": "c379d728-9870-4fca-cbca-24658e0a12ef"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/wassal/trust-wassal/')\n",
    "\n",
    "from trust.utils.models.resnet import ResNet18\n",
    "from trust.utils.models.resnet import ResNet50\n",
    "from trust.utils.custom_dataset_medmnist import load_biodataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from trust.strategies.smi import SMI\n",
    "from trust.strategies.random_sampling import RandomSampling\n",
    "from trust.strategies.wassal import WASSAL\n",
    "\n",
    "sys.path.append('/home/wassal/distil')\n",
    "from distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
    "from distil.active_learning_strategies.badge import BADGE\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from trust.utils.utils import *\n",
    "from trust.utils.viz import tsne_smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec462346",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c36c6b4",
   "metadata": {
    "id": "ClNjNvIX8lLT"
   },
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'ResNet50':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet50(num_cls)\n",
    "        else:\n",
    "            model = models.resnet50()\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def displayTable(val_err_log, tst_err_log):\n",
    "    col1 = [str(i) for i in range(10)]\n",
    "    val_acc = [str(100-i) for i in val_err_log]\n",
    "    tst_acc = [str(100-i) for i in tst_err_log]\n",
    "    table = [col1, val_acc, tst_acc]\n",
    "    table = map(list, zip(*table))\n",
    "    print(tabulate(table, headers=['Class', 'Val Accuracy', 'Test Accuracy'], tablefmt='orgtbl'))\n",
    "\n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "#         print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    displayTable(val_err_log, tst_err_log)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.astype(np.float))[subset])\n",
    "    else:\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "#     print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    aug_trainloader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True, pin_memory=True)\n",
    "    return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set, val_class_err_idxs, imb_cls_idx, miscls):\n",
    "    miscls_idx = []\n",
    "    if(miscls):\n",
    "        for i in range(len(val_class_err_idxs)):\n",
    "            if i in imb_cls_idx:\n",
    "                miscls_idx += val_class_err_idxs[i]\n",
    "        print(\"Total misclassified examples from imbalanced classes (Size of query set): \", len(miscls_idx))\n",
    "    else:\n",
    "        for i in imb_cls_idx:\n",
    "            imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "            miscls_idx += imb_cls_samples\n",
    "        print(\"Total samples from imbalanced classes as targets (Size of query set): \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx), val_set.targets[miscls_idx]\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    if str(type(lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        subset_cls = torch.Tensor(lake_set.targets.astype(np.float))[subset]\n",
    "    else:\n",
    "        subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "def print_final_results(res_dict, sel_cls_idx):\n",
    "    print(\"Gain in overall test accuracy: \", res_dict['test_acc'][1]-res_dict['test_acc'][0])\n",
    "    bf_sel_cls_acc = np.array(res_dict['all_class_acc'][0])[sel_cls_idx]\n",
    "    af_sel_cls_acc = np.array(res_dict['all_class_acc'][1])[sel_cls_idx]\n",
    "    print(\"Gain in targeted test accuracy: \", np.mean(af_sel_cls_acc-bf_sel_cls_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02859f",
   "metadata": {},
   "source": [
    "# Data, Model & Experimental Settings\n",
    "The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images and test set contains 10,000 images. We will use custom_dataset() function in Trust to simulated a class imbalance scenario using the split_cfg dictionary given below. We then use a ResNet18 model as our task DNN and train it on the simulated imbalanced version of the CIFAR-10 dataset. Next we perform targeted selection using various SMI functions and compare their gain in overall accuracy as well as on the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ce679b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkbsfjml8lLX",
    "outputId": "283a4f46-1575-40a6-e915-88d2d7a4e793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_cfg: {'per_class_train': {0: 100, 1: 5}, 'per_class_val': {0: 20, 1: 20}, 'per_class_lake': {0: 1050, 1: 53}, 'per_class_test': {0: 200, 1: 200}, 'sel_cls_idx': [1], 'per_imbclass_train': {0: 100, 1: 5}, 'per_imbclass_val': {0: 20, 1: 20}, 'per_imbclass_lake': {0: 1050, 1: 53}, 'per_imbclass_test': {0: 200, 1: 200}}\n"
     ]
    }
   ],
   "source": [
    "feature = \"classimb\"\n",
    "device_id = 0\n",
    "run=\"test_run\"\n",
    "# datadir = 'data/'\n",
    "datadir = '/data/medmnist' #contains the npz file of the data_name dataset listed below\n",
    "data_name = 'pneumoniamnist'\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.0003\n",
    "computeClassErrorLog = True\n",
    "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "miscls = True #Set to True if only the misclassified examples from the imbalanced classes is to be used\n",
    "embedding_type = \"gradients\" #Type of the representation to use (gradients/features)\n",
    "num_cls = 2\n",
    "budget = 10\n",
    "visualize_tsne = False\n",
    "split_cfg = {\n",
    "             \"per_class_train\":{0:100,1:5}, \n",
    "             \"per_class_val\":{0:20,1:20}, \n",
    "             \"per_class_lake\":{0:1050,1:53},\n",
    "             \"per_class_test\":{0:200,1:200},\n",
    "             \"sel_cls_idx\":[1], \n",
    "             \"per_imbclass_train\":{0:100,1:5}, \n",
    "             \"per_imbclass_val\":{0:20,1:20}, \n",
    "             \"per_imbclass_lake\":{0:1050,1:53},\n",
    "             \"per_imbclass_test\":{0:200,1:200}}\n",
    "print(\"split_cfg:\",split_cfg)\n",
    "initModelPath = \"./\"+data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"sel_cls_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2a173",
   "metadata": {
    "id": "9qKXzKtd8lLZ"
   },
   "source": [
    "# Targeted Selection Algorithm\n",
    "1. Given: Initial Labeled set of Examples: ùê∏, large unlabeled dataset: ùëà, A target subset/slice where we want to improve accuracy: ùëá, Loss function ùêø for learning\n",
    "2. Train model with loss $\\mathcal L$ on labeled set $E$ and obtain parameters $\\theta_E$\n",
    "3. Compute the gradients $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in U\\}$ (using hypothesized labels) and $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in T\\}$. \n",
    "(This notebook uses gradients for representation. However, any other representation can be used. Trust also supports using features via the API.)\n",
    "4. Compute the similarity kernels $S$ (this includes kernel of the elements within $U$, within $T$ and between $U$ and $T$) and define a submodular function $f$ and diversity function $g$\n",
    "5. Compute subset $\\hat{A}$ by mazximizing the SMI function: $\\hat{A} \\gets \\max_{A \\subseteq U, |A|\\leq k} I_f(A;T) + \\gamma g(A)$\n",
    "6. Obtain the labels of the elements in $A^*$: $L(\\hat{A})$\n",
    "7. Train a model on the combined labeled set $E \\cup L(\\hat{A})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b510c9",
   "metadata": {
    "id": "mMSfzzqM8lLZ"
   },
   "outputs": [],
   "source": [
    "def run_targeted_selection(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "\n",
    "    #load the dataset in the class imbalance setting\n",
    "    train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_biodataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "    print(\"Indices of randomly selected classes for imbalance: \", sel_cls_idx)\n",
    "    \n",
    "    #Set batch size for train, validation and test datasets\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    #Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    num_rounds=2 #The first round is for training the initial model and the second round is to train the final model\n",
    "    fulltrn_losses = np.zeros(num_rounds)\n",
    "    val_losses = np.zeros(num_rounds)\n",
    "    tst_losses = np.zeros(num_rounds)\n",
    "    timing = np.zeros(num_rounds)\n",
    "    val_acc = np.zeros(num_rounds)\n",
    "    full_trn_acc = np.zeros(num_rounds)\n",
    "    tst_acc = np.zeros(num_rounds)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    all_logs_dir = '/home/wassal/trust-wassal/tutorials/results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir]) #Uncomment for saving results\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_rounds:' + str(num_rounds) + '_runs' + str(run)\n",
    "\n",
    "    #Create a dictionary for storing results and the experimental setting\n",
    "    res_dict = {\"dataset\":data_name, \n",
    "                \"feature\":feature, \n",
    "                \"sel_func\":sf,\n",
    "                \"sel_budget\":budget, \n",
    "                \"num_selections\":num_rounds-1, \n",
    "                \"model\":model_name, \n",
    "                \"learning_rate\":learning_rate, \n",
    "                \"setting\":split_cfg, \n",
    "                \"all_class_acc\":None, \n",
    "                \"test_acc\":[],\n",
    "                \"sel_per_cls\":[], \n",
    "                \"sel_cls_idx\":sel_cls_idx}\n",
    "    \n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device, embedding_type)\n",
    "    model1 = create_model(model_name, num_cls, device, embedding_type)\n",
    "    strategy_args = {'batch_size': 20, 'device':device, 'embedding_type':'gradients', 'keep_embedding':True}\n",
    "    unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "    \n",
    "    if(strategy == \"AL\"):\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_sel = GradMatchActive(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SIM\"):\n",
    "        strategy_args['smi_function'] = sf\n",
    "        strategy_sel = SMI(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"random\"):\n",
    "        strategy_sel = RandomSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"WASSAL\"):\n",
    "        strategy_sel = WASSAL(train_set, unlabeled_lake_set,val_set, model,num_cls,strategy_args)\n",
    "\n",
    "        \n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"Initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)): #Read the initial trained model if it exists\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    final_tst_predictions = []\n",
    "                    final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                        final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "                continue\n",
    "        else:\n",
    "            #Remove true labels from the unlabeled dataset, the hypothesized labels are computed when select is called\n",
    "            unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "            strategy_sel.update_data(train_set, unlabeled_lake_set)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([100-x for x in tst_err_log])\n",
    "                val_csvlog.append([100-x for x in val_err_log])\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set, miscls_set_targets = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                        strategy_sel.update_queries(miscls_set)\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\"):\n",
    "                    miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                    strategy_sel.update_queries(miscls_set)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "            \n",
    "            strategy_sel.update_model(model)\n",
    "            subset = strategy_sel.select(budget)\n",
    "            print(\"#### Selection Complete, Now re-training with augmented subset ####\")\n",
    "            if(visualize_tsne):\n",
    "                tsne_plt = tsne_smi(strategy_sel.unlabeled_data_embedding.cpu(),\n",
    "                                    lake_set.targets,\n",
    "                                    strategy_sel.query_embedding.cpu(),\n",
    "                                    miscls_set_targets,\n",
    "                                    subset)\n",
    "                print(\"Computed TSNE plot of the selection\")\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set, true_lake_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" unlabeled set: \", len(lake_set), \" val set: \", len(val_set))\n",
    "    \n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            model = create_model(model_name, num_cls, device, strategy_args['embedding_type'])\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "#         while(num_ep<150):\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<100):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader): #Compute Train accuracy\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader): #Compute Val accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader): #Compute test accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "            print(\"Gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][i-1])\n",
    "        if(i==0): \n",
    "            print(\"Saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "            \n",
    "    #Compute the statistics of the final model\n",
    "    if(computeErrorLog):\n",
    "        print(\"**** Final Metrics after Targeted Learning ****\")\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([100-x for x in tst_err_log])\n",
    "        val_csvlog.append([100-x for x in val_err_log])\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "        with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    #Print overall acc improvement and rare class acc improvement, show that TL selected relevant points in space, is possible show some images\n",
    "#     print_final_results(res_dict, sel_cls_idx)\n",
    "    print(\"Total gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][0])\n",
    "    \n",
    "#     tsne_plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfc956",
   "metadata": {},
   "source": [
    "# WASSAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f962e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb//10/test_run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassal/.wassalvenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wassal/.wassalvenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             40 |           36.15 |\n",
      "There are 1103 Unlabeled dataset\n",
      "Epoch: 0\n",
      "Batchwise loss: 90.37471771240234\n",
      "Avg loss: 90.37471771240234\n",
      "Epoch: 1\n",
      "Batchwise loss: 76.21438598632812\n",
      "Avg loss: 76.21438598632812\n",
      "Epoch: 2\n",
      "Batchwise loss: 75.7072525024414\n",
      "Avg loss: 75.7072525024414\n",
      "Epoch: 3\n",
      "Batchwise loss: 75.07569885253906\n",
      "Avg loss: 75.07569885253906\n",
      "Epoch: 4\n",
      "Batchwise loss: 74.13797760009766\n",
      "Avg loss: 74.13797760009766\n",
      "Epoch: 5\n",
      "Batchwise loss: 74.71569061279297\n",
      "Avg loss: 74.71569061279297\n",
      "Epoch: 6\n",
      "Batchwise loss: 74.5794677734375\n",
      "Avg loss: 74.5794677734375\n",
      "Epoch: 7\n",
      "Batchwise loss: 75.15370178222656\n",
      "Avg loss: 75.15370178222656\n",
      "Epoch: 8\n",
      "Batchwise loss: 74.22152709960938\n",
      "Avg loss: 74.22152709960938\n",
      "Epoch: 9\n",
      "Batchwise loss: 74.25835418701172\n",
      "Avg loss: 74.25835418701172\n",
      "length of unlabelled dataset 1103\n",
      "Totals Probability of the budget: tensor(0.0159, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "selected indices len  10\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.23175945784896612 1.0 1.3618405051529408 0.85 3.5724017322063446 0.780448717948718 1.3142671585083008\n",
      "Gain in accuracy:  18.91025641025641\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           91.45 |\n",
      "|       1 |             70 |           70    |\n",
      "Total gain in accuracy:  18.91025641025641\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args['budget'] = 10\n",
    "args['num_rounds'] = 6\n",
    "args['classifier_learning_rate'] = 0.002\n",
    "args['verbose']=True\n",
    "run_targeted_selection(data_name,\n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               args['budget'], \n",
    "               split_cfg,\n",
    "               args['classifier_learning_rate'],\n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"WASSAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240bc93",
   "metadata": {},
   "source": [
    "# Submodular Mutual Information (SMI)\n",
    "\n",
    "We let $V$ denote the ground-set of $n$ data points $V = \\{1, 2, 3,...,n \\}$ and a set function $f:\n",
    " 2^{V} \\xrightarrow{} \\Re$. Given a set of items $A, B \\subseteq V$, the submodular mutual information (MI)[1,3] is defined as $I_f(A; B) = f(A) + f(B) - f(A \\cup B)$. Intuitively, this measures the similarity between $B$ and $A$ and we refer to $B$ as the query set.\n",
    "\n",
    "In [2], they extend MI to handle the case when the target can come from an auxiliary set $V^{\\prime}$ different from the ground set $V$. For targeted data subset selection, $V$ is the source set of data instances and the target is a subset of data points (validation set or the specific set of examples of interest).\n",
    "Let $\\Omega  = V \\cup V^{\\prime}$. We define a set function $f: 2^{\\Omega} \\rightarrow \\Re$. Although $f$ is defined on $\\Omega$, the discrete optimization problem will only be defined on subsets $A \\subseteq V$. To find an optimal subset given a query set $Q \\subseteq V^{\\prime}$, we can define $g_{Q}(A) = I_f(A; Q)$, $A \\subseteq V$ and maximize the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3045e5",
   "metadata": {
    "id": "PeujlNrf8lLa"
   },
   "source": [
    "# FL1MI\n",
    "\n",
    "In the first variant of FL, we set the unlabeled dataset to be $V$. The SMI instantiation of FL1MI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\sum_{i \\in V}\\min(\\max_{j \\in A}s_{ij}, \\eta \\max_{j \\in Q}sq_{ij})\n",
    "\\end{align}\n",
    "\n",
    "The first term in the min(.) of FL1MI models diversity, and the second term models query relevance. An increase in the value of $\\eta$ causes the resulting summary to become more relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a96a686",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "d978b42c8c2d4ce69ca7a3233094ba73",
      "14a33535616d4fa993d20e525666af04",
      "bb6c0909711948b98f0a0b767cb9201e",
      "6792820b58df4d2b86386c29aaad1a9f",
      "9d386f74f6c4469bbbf1ac6b2a49afed",
      "4ee6a0573c6845458996a59679ac6068",
      "0074c4d4c28d4dad87a3eaaac8458479",
      "cbe081bb90d04faaae98fef1c9921180"
     ]
    },
    "id": "QHn0QzY78lLa",
    "outputId": "a203ea99-fb5b-4bfa-99e5-699ef0715d16",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/fl1mi/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             25 |           36.15 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.6114391982555389 0.991304347826087 2.0468102768063545 0.725 4.649634003639221 0.6634615384615384 1.2606401443481445\n",
      "Gain in accuracy:  7.211538461538453\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.29 |\n",
      "|       1 |             45 |           47.18 |\n",
      "Total gain in accuracy:  7.211538461538453\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fc84e",
   "metadata": {
    "id": "kp9_ZU7I8lLa"
   },
   "source": [
    "# FL2MI\n",
    "\n",
    "In the V2 variant, we set $D$ to be $V \\cup Q$. The SMI instantiation of FL2MI can be defined as:\n",
    "\\begin{align} \\label{eq:FL2MI}\n",
    "I_f(A;Q)=\\sum_{i \\in Q} \\max_{j \\in A} sq_{ij} + \\eta\\sum_{i \\in A} \\max_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "FL2MI is very intuitive for query relevance as well. It measures the representation of data points that are the most relevant to the query set and vice versa. It can also be thought of as a bidirectional representation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e29177",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZKsblhs8lLa",
    "outputId": "5983fdfb-6541-43e9-a0f0-304ddfa543b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/fl2mi/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             25 |           36.15 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  15\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5876662507653236 0.991304347826087 1.1894743740558624 0.85 4.099206119775772 0.7307692307692307 1.2550814151763916\n",
      "Gain in accuracy:  13.94230769230768\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           96.58 |\n",
      "|       1 |             70 |           58.97 |\n",
      "Total gain in accuracy:  13.94230769230768\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog, \n",
    "               \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ea94d",
   "metadata": {
    "id": "9f6qyrpA8lLc"
   },
   "source": [
    "# GCMI\n",
    "\n",
    "The SMI instantiation of graph-cut (GCMI) is defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=2\\sum_{i \\in A} \\sum_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "Since maximizing GCMI maximizes the joint pairwise sum with the query set, it will lead to a subset similar to the query set $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc6e91d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MM9copHy8lLc",
    "outputId": "1c5e4fe8-2f3c-4137-9f64-c728e00c34d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/gcmi/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             40 |           36.15 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  12\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5223380625247955 0.991304347826087 1.4605324193835258 0.775 4.134872704744339 0.7355769230769231 1.3610022068023682\n",
      "Gain in accuracy:  14.42307692307692\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             55 |           59.23 |\n",
      "Total gain in accuracy:  14.42307692307692\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07323a40",
   "metadata": {
    "id": "mGDPAjDR8lLc"
   },
   "source": [
    "# LOGDETMI\n",
    "\n",
    "The SMI instantiation of LogDetMI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\log\\det(S_{A}) -\\log\\det(S_{A} - \\eta^2 S_{A,Q}S_{Q}^{-1}S_{A,Q}^T)\n",
    "\\end{align}\n",
    "$S_{A, B}$ denotes the cross-similarity matrix between the items in sets $A$ and $B$. The similarity matrix in constructed in such a way that the cross-similarity between $A$ and $Q$ is multiplied by $\\eta$ to control the trade-off between query-relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ad455b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1QbcrJk8lLd",
    "outputId": "1a7604b8-dee7-468f-d9bf-7e8b202c18d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/logdetmi/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             35 |           36.15 |\n",
      "Total misclassified examples from imbalanced classes (Size of query set):  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5161721222102642 0.991304347826087 2.0620662793517113 0.65 5.522513210773468 0.6025641025641025 1.156808614730835\n",
      "Gain in accuracy:  1.1217948717948687\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           98.29 |\n",
      "|       1 |             30 |           37.44 |\n",
      "Total gain in accuracy:  1.1217948717948687\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec7d2d",
   "metadata": {
    "id": "3dZqCZ1v8lLe"
   },
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9389578a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlbOpp438lLe",
    "outputId": "f754727a-dfb5-4853-d4c1-8a96115182db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/random/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             35 |           36.15 |\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.38545225095003843 0.991304347826087 3.733988143503666 0.525 7.8451844453811646 0.41346153846153844 1.5742592811584473\n",
      "Gain in accuracy:  -17.78846153846154\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |          100    |\n",
      "|       1 |              5 |            6.15 |\n",
      "Total gain in accuracy:  -17.78846153846154\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fddb5",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6164c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/us/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             25 |           36.15 |\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5512277074158192 0.991304347826087 1.7868801094591618 0.75 4.6911024153232574 0.7003205128205128 1.3912034034729004\n",
      "Gain in accuracy:  10.897435897435884\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.01 |\n",
      "|       1 |             50 |           53.85 |\n",
      "Total gain in accuracy:  10.897435897435884\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc215ed",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e524cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /data/medmnist/pneumoniamnist.npz\n",
      "pneumoniamnist Custom dataset stats: Train size:  105 Val size:  40 Lake size:  1103\n",
      "Indices of randomly selected classes for imbalance:  [1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/pneumoniamnist/classimb/badge/10/test_run\n",
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  ./pneumoniamnist_ResNet18_0.0003_[1]\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           97.44 |\n",
      "|       1 |             35 |           36.15 |\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  115  unlabeled set:  1093  val set:  40\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8569695428013802 1.0 1.7770193591713905 0.7 3.703550010919571 0.7371794871794872 1.0607223510742188\n",
      "Gain in accuracy:  14.583333333333343\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |            100 |           96.58 |\n",
      "|       1 |             40 |           60    |\n",
      "Total gain in accuracy:  14.583333333333343\n"
     ]
    }
   ],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf8b9c",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asnani. Submodular combinatorialinformation measures with applications in machine learning.arXiv preprint arXiv:2006.15412,2020\n",
    "\n",
    "\n",
    "[2] Kaushal V, Kothawade S, Ramakrishnan G, Bilmes J, Iyer R. PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. arXiv preprint arXiv:2103.00128. 2021 Feb 27.\n",
    "\n",
    "\n",
    "[3] Anupam Gupta and Roie Levin. The online submodular cover problem. InACM-SIAM Symposiumon Discrete Algorithms, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a22fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "submodlib_cifar_classimb.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0074c4d4c28d4dad87a3eaaac8458479": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a33535616d4fa993d20e525666af04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee6a0573c6845458996a59679ac6068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6792820b58df4d2b86386c29aaad1a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe081bb90d04faaae98fef1c9921180",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0074c4d4c28d4dad87a3eaaac8458479",
      "value": " 170499072/? [00:07&lt;00:00, 23215900.76it/s]"
     }
    },
    "9d386f74f6c4469bbbf1ac6b2a49afed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bb6c0909711948b98f0a0b767cb9201e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee6a0573c6845458996a59679ac6068",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d386f74f6c4469bbbf1ac6b2a49afed",
      "value": 170498071
     }
    },
    "cbe081bb90d04faaae98fef1c9921180": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d978b42c8c2d4ce69ca7a3233094ba73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb6c0909711948b98f0a0b767cb9201e",
       "IPY_MODEL_6792820b58df4d2b86386c29aaad1a9f"
      ],
      "layout": "IPY_MODEL_14a33535616d4fa993d20e525666af04"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
