{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087af8f6",
   "metadata": {
    "id": "Y4YlT-8B8lLN"
   },
   "source": [
    "# Targeted Selection Demo For Biomedical Datasets With Rare Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fc69c",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b905244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMIAA-Ua8lLR",
    "outputId": "c379d728-9870-4fca-cbca-24658e0a12ef"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/wassal/trust-wassal/')\n",
    "\n",
    "from trust.utils.models.resnet import ResNet18\n",
    "from trust.utils.models.resnet import ResNet50\n",
    "from trust.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from trust.strategies.smi import SMI\n",
    "from trust.strategies.random_sampling import RandomSampling\n",
    "from trust.strategies.wassal import WASSAL\n",
    "from trust.strategies.wassal_refrain import WASSAL_P\n",
    "\n",
    "sys.path.append('/home/wassal/distil')\n",
    "from distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
    "from distil.active_learning_strategies.badge import BADGE\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from trust.utils.utils import *\n",
    "from trust.utils.viz import tsne_smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec462346",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c36c6b4",
   "metadata": {
    "id": "ClNjNvIX8lLT"
   },
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, querys) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'ResNet50':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet50(num_cls)\n",
    "        else:\n",
    "            model = models.resnet50()\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def displayTable(val_err_log, tst_err_log):\n",
    "    col1 = [str(i) for i in range(10)]\n",
    "    val_acc = [str(100-i) for i in val_err_log]\n",
    "    tst_acc = [str(100-i) for i in tst_err_log]\n",
    "    table = [col1, val_acc, tst_acc]\n",
    "    table = map(list, zip(*table))\n",
    "    print(tabulate(table, headers=['Class', 'Val Accuracy', 'Test Accuracy'], tablefmt='orgtbl'))\n",
    "\n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "#         print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    displayTable(val_err_log, tst_err_log)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.astype(np.float))[subset])\n",
    "    else:\n",
    "        lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    if str(type(true_lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.astype(np.float))[remain_lake_idx])\n",
    "    else:\n",
    "        remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "#     print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    aug_train_set = ConcatWithTargets(train_set, lake_ss)\n",
    "    aug_trainloader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True, pin_memory=True)\n",
    "    return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set,imb_cls_idx):\n",
    "    \n",
    "    miscls_idx = []\n",
    "    \n",
    "    \n",
    "    for i in imb_cls_idx:\n",
    "        imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        miscls_idx += imb_cls_samples\n",
    "    print(\"Total samples from imbalanced classes as Queries (Size of query set): \", len(miscls_idx))\n",
    "    return SubsetWithTargets(val_set, miscls_idx, val_set.targets[miscls_idx])\n",
    "\n",
    "def getPrivateSet(val_set,imb_cls_idx):\n",
    "    # Get all the indices in the val_set\n",
    "    all_idx = list(range(len(val_set.targets)))\n",
    "    miscls_idx = []\n",
    "    \n",
    "    \n",
    "    for i in imb_cls_idx:\n",
    "        imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        miscls_idx += imb_cls_samples\n",
    "     # Get indices that aren't in the query class samples\n",
    "    private_idx = list(set(all_idx) - set(miscls_idx))\n",
    "    print(\"Total samples from imbalanced classes as Private (Size of private set): \", len(private_idx))\n",
    "    return SubsetWithTargets(val_set, private_idx, val_set.targets[private_idx])\n",
    "\n",
    "\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    if str(type(lake_set.targets)) == \"<class 'numpy.ndarray'>\":\n",
    "        subset_cls = torch.Tensor(lake_set.targets.astype(np.float))[subset]\n",
    "    else:\n",
    "        subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "def print_final_results(res_dict, sel_cls_idx):\n",
    "    print(\"Gain in overall test accuracy: \", res_dict['test_acc'][1]-res_dict['test_acc'][0])\n",
    "    bf_sel_cls_acc = np.array(res_dict['all_class_acc'][0])[sel_cls_idx]\n",
    "    af_sel_cls_acc = np.array(res_dict['all_class_acc'][1])[sel_cls_idx]\n",
    "    print(\"Gain in targeted test accuracy: \", np.mean(af_sel_cls_acc-bf_sel_cls_acc))\n",
    "\n",
    "def analyze_simplex(args, unlabeled_set, simplex_query):\n",
    "    print(\"======== analysis on simplex =========\")\n",
    "    unlabeled_loader = torch.utils.data.DataLoader(dataset=unlabeled_set, batch_size=len(unlabeled_set), shuffle=False)\n",
    "    u_imgs, u_labels = next(iter(unlabeled_loader))\n",
    "    u_imgs, u_labels = u_imgs.to(args['device']), u_labels.to(args['device'])\n",
    "    nz_query_idx = simplex_query.nonzero()\n",
    "\n",
    "    # Using a loop to accommodate an array of target values\n",
    "    total_correctly_identified = 0\n",
    "    for query_value in args['target']:\n",
    "        num_nz_query = (u_labels[nz_query_idx] == query_value).nonzero().shape[0]\n",
    "        total_correctly_identified += num_nz_query\n",
    "    print(\"no of query labels identified correctly: {}/{}\".format(total_correctly_identified, nz_query_idx.shape[0]))\n",
    "\n",
    "    total_query_weight = 0\n",
    "    for query_value in args['target']:\n",
    "        query_idx = torch.where(u_labels == query_value)\n",
    "        query_weight = torch.sum(simplex_query[query_idx])\n",
    "        total_query_weight += query_weight\n",
    "    print(\"Weight of Query samples in simplex_query: {}\".format(total_query_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02859f",
   "metadata": {},
   "source": [
    "# Data, Model & Experimental Settings\n",
    "The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images and test set contains 10,000 images. We will use custom_dataset() function in Trust to simulated a class imbalance scenario using the split_cfg dictionary given below. We then use a ResNet18 model as our task DNN and train it on the simulated imbalanced version of the CIFAR-10 dataset. Next we perform targeted selection using various SMI functions and compare their gain in overall accuracy as well as on the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ce679b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkbsfjml8lLX",
    "outputId": "283a4f46-1575-40a6-e915-88d2d7a4e793"
   },
   "outputs": [],
   "source": [
    "feature = \"classimb\"\n",
    "device_id = 0\n",
    "run=\"exp1\"\n",
    "# datadir = 'data/'\n",
    "datadir = '/data' #contains the npz file of the data_name dataset listed below\n",
    "data_name = 'cifar10'\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.001\n",
    "computeClassErrorLog = True\n",
    "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "miscls = False #Set to True if only the misclassified examples from the imbalanced classes is to be used\n",
    "embedding_type = \"features\" #Type of the representation to use (gradients/features)\n",
    "budget = 400\n",
    "visualize_tsne = False\n",
    "num_cls = 10\n",
    "\n",
    "\n",
    "split_cfg = {\"num_cls_imbalance\":2, #Number of rare classes\n",
    "             \"per_imbclass_train\":50, #Number of samples per rare class in the train dataset\n",
    "             \"per_imbclass_val\":200, #Number of samples per rare class in the validation dataset\n",
    "             \"per_imbclass_lake\":150, #Number of samples per rare class in the unlabeled dataset\n",
    "             \"per_class_train\":1000,  #Number of samples per unrare class in the train dataset\n",
    "             \"per_class_val\":200, #Number of samples per unrare class in the validation dataset\n",
    "             \"per_class_lake\":3000} #Number of samples per unrare class in the unlabeled dataset\n",
    "initModelPath = \"/home/wassal/trust-wassal/tutorials/results/\"+data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_imbclass_train\"]) + \"_\" + str(split_cfg[\"per_class_train\"]) + \"_\" + str(split_cfg[\"num_cls_imbalance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2a173",
   "metadata": {
    "id": "9qKXzKtd8lLZ"
   },
   "source": [
    "# Targeted Selection Algorithm\n",
    "1. Given: Initial Labeled set of Examples: 𝐸, large unlabeled dataset: 𝑈, A target subset/slice where we want to improve accuracy: 𝑇, Loss function 𝐿 for learning\n",
    "2. Train model with loss $\\mathcal L$ on labeled set $E$ and obtain parameters $\\theta_E$\n",
    "3. Compute the gradients $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in U\\}$ (using hypothesized labels) and $\\{\\nabla_{\\theta_E} \\mathcal L(x_i, y_i), i \\in T\\}$. \n",
    "(This notebook uses gradients for representation. However, any other representation can be used. Trust also supports using features via the API.)\n",
    "4. Compute the similarity kernels $S$ (this includes kernel of the elements within $U$, within $T$ and between $U$ and $T$) and define a submodular function $f$ and diversity function $g$\n",
    "5. Compute subset $\\hat{A}$ by mazximizing the SMI function: $\\hat{A} \\gets \\max_{A \\subseteq U, |A|\\leq k} I_f(A;T) + \\gamma g(A)$\n",
    "6. Obtain the labels of the elements in $A^*$: $L(\\hat{A})$\n",
    "7. Train a model on the combined labeled set $E \\cup L(\\hat{A})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b510c9",
   "metadata": {
    "id": "mMSfzzqM8lLZ"
   },
   "outputs": [],
   "source": [
    "def run_targeted_selection(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "\n",
    "    #load the dataset in the class imbalance setting\n",
    "    train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "    print(\"Indices of randomly selected classes for imbalance: \", sel_cls_idx)\n",
    "    \n",
    "    #Set batch size for train, validation and test datasets\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = len(train_set)\n",
    "    val_batch_size = len(val_set)\n",
    "    tst_batch_size = len(test_set)\n",
    "\n",
    "    #Create dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    num_rounds=2 #The first round is for training the initial model and the second round is to train the final model\n",
    "    fulltrn_losses = np.zeros(num_rounds)\n",
    "    val_losses = np.zeros(num_rounds)\n",
    "    tst_losses = np.zeros(num_rounds)\n",
    "    timing = np.zeros(num_rounds)\n",
    "    val_acc = np.zeros(num_rounds)\n",
    "    full_trn_acc = np.zeros(num_rounds)\n",
    "    tst_acc = np.zeros(num_rounds)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    all_logs_dir = '/home/wassal/trust-wassal/tutorials/results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir]) #Uncomment for saving results\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_rounds:' + str(num_rounds) + '_runs' + str(run)\n",
    "\n",
    "    #Create a dictionary for storing results and the experimental setting\n",
    "    res_dict = {\"dataset\":data_name, \n",
    "                \"feature\":feature, \n",
    "                \"sel_func\":sf,\n",
    "                \"sel_budget\":budget, \n",
    "                \"num_selections\":num_rounds-1, \n",
    "                \"model\":model_name, \n",
    "                \"learning_rate\":learning_rate, \n",
    "                \"setting\":split_cfg, \n",
    "                \"all_class_acc\":None, \n",
    "                \"test_acc\":[],\n",
    "                \"sel_per_cls\":[], \n",
    "                \"sel_cls_idx\":sel_cls_idx.tolist()}\n",
    "    \n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device, embedding_type)\n",
    "   \n",
    "    strategy_args = {'batch_size': 1000, 'device':device, 'embedding_type':'features', 'keep_embedding':True}\n",
    "    unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "    \n",
    "    \n",
    "    if(strategy == \"SIM\"):\n",
    "        strategy_args['smi_function'] = sf\n",
    "        for_query_set = getQuerySet(train_set,sel_cls_idx)\n",
    "        strategy_sel = SMI(train_set, unlabeled_lake_set, for_query_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SCMI\"):\n",
    "        strategy_args['scmi_function'] = sf\n",
    "        strategy_sel = SCMI(train_set, unlabeled_lake_set, for_query_set, val_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SCG\"):\n",
    "        strategy_args['scg_function'] = sf\n",
    "        strategy_sel = SCG(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"random\"):\n",
    "        strategy_sel = RandomSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"WASSAL\"):\n",
    "        for_query_set = getQuerySet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "        strategy_sel = WASSAL(train_set, unlabeled_lake_set,for_query_set, model,num_cls,strategy_args)\n",
    "    if(strategy == \"WASSAL_P\"):\n",
    "        for_query_set = getQuerySet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "        for_private_set = getPrivateSet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "        strategy_sel = WASSAL_P(train_set, unlabeled_lake_set,for_query_set, for_private_set,model,num_cls,strategy_args)\n",
    "\n",
    "        \n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"Initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)): #Read the initial trained model if it exists\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    final_tst_predictions = []\n",
    "                    final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                        final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "                continue\n",
    "        else:\n",
    "            #Remove true labels from the unlabeled dataset, the hypothesized labels are computed when select is called\n",
    "            unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "            strategy_sel.update_data(train_set, unlabeled_lake_set)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([100-x for x in tst_err_log])\n",
    "                val_csvlog.append([100-x for x in val_err_log])\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with queries\n",
    "                        for_query_set = getQuerySet(ConcatWithTargets(train_set, val_set),sel_cls_idx)\n",
    "                        strategy_sel.update_queries(for_query_set)\n",
    "                        print('size of query set',len(for_query_set))\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\"):\n",
    "                    miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                    strategy_sel.update_queries(miscls_set)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "            elif(strategy==\"WASSAL\"):\n",
    "                #concatina the train and val sets\n",
    "                for_query_set = getQuerySet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "                strategy_sel.update_queries(for_query_set)\n",
    "                print('size of query set',len(for_query_set))\n",
    "            elif(strategy==\"WASSAL_P\"):\n",
    "                #concatina the train and val sets\n",
    "                for_query_set = getQuerySet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "                for_private_set=getPrivateSet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n",
    "                strategy_sel.update_queries(for_query_set)\n",
    "                strategy_sel.update_privates(for_private_set)\n",
    "                print('size of query set',len(for_query_set))\n",
    "\n",
    "            \n",
    "            strategy_sel.update_model(model)\n",
    "            if(strategy==\"WASSAL\"):\n",
    "                subset,simplex_query = strategy_sel.select(budget)\n",
    "                temp_args={}\n",
    "                temp_args['device'] = device\n",
    "                temp_args['target'] = sel_cls_idx\n",
    "                #analyze_simplex(temp_args,lake_set,simplex_query)\n",
    "            elif(strategy==\"WASSAL_P\"):\n",
    "                subset,simplex_query,simplex_private = strategy_sel.select(budget)\n",
    "                temp_args={}\n",
    "                temp_args['device'] = device\n",
    "                temp_args['target'] = sel_cls_idx\n",
    "                #analyze_simplex(temp_args,lake_set,simplex_query)\n",
    "            else:\n",
    "                subset = strategy_sel.select(budget)\n",
    "            print(\"#### Selection Complete, Now re-training with augmented subset ####\")\n",
    "            if(visualize_tsne):\n",
    "                tsne_plt = tsne_smi(strategy_sel.unlabeled_data_embedding.cpu(),\n",
    "                                    lake_set.targets,\n",
    "                                    strategy_sel.query_embedding.cpu(),\n",
    "                                    sel_cls_idx,\n",
    "                                    subset)\n",
    "                print(\"Computed TSNE plot of the selection\")\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set, true_lake_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" unlabeled set: \", len(lake_set), \" val set: \", len(val_set))\n",
    "    \n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            #model = create_model(model_name, num_cls, device, strategy_args['embedding_type'])\n",
    "            #optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "#         while(num_ep<150):\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<100):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader): #Compute Train accuracy\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader): #Compute Val accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader): #Compute test accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i]*100)\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "            print(\"Gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][i-1])\n",
    "        if(i==0): \n",
    "            print(\"Saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "            \n",
    "    #Compute the statistics of the final model\n",
    "    if(computeErrorLog):\n",
    "        print(\"**** Final Metrics after Targeted Learning ****\")\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([100-x for x in tst_err_log])\n",
    "        val_csvlog.append([100-x for x in val_err_log])\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "        with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    #Print overall acc improvement and rare class acc improvement, show that TL selected relevant points in space, is possible show some images\n",
    "    print_final_results(res_dict, sel_cls_idx)\n",
    "    print(\"Total gain in accuracy: \",res_dict['test_acc'][i]-res_dict['test_acc'][0])\n",
    "    \n",
    "#     tsne_plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93a22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget  50  Strategy  WASSAL  Method  WASSAL\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  8100 Val size:  2000 Lake size:  24300\n",
      "Indices of randomly selected classes for imbalance:  [8 1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/cifar10/classimb/WASSAL/50/exp1\n",
      "Total samples from imbalanced classes as Queries (Size of query set):  500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassal/.wassalvenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wassal/.wassalvenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training epoch\n",
      "Init model loaded from disk, skipping init training:  /home/wassal/trust-wassal/tutorials/results/cifar10_ResNet18_0.001_50_1000_2\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |           65.5 |            59.8 |\n",
      "|       1 |            0   |             0   |\n",
      "|       2 |           24.5 |            26.6 |\n",
      "|       3 |           23   |            25.4 |\n",
      "|       4 |           39   |            32.3 |\n",
      "|       5 |           28.5 |            33.2 |\n",
      "|       6 |           45   |            44.7 |\n",
      "|       7 |           37   |            38.7 |\n",
      "|       8 |            0   |             0   |\n",
      "|       9 |           61.5 |            61   |\n",
      "Total samples from imbalanced classes as Queries (Size of query set):  500\n",
      "size of query set 500\n",
      "There are 24300 Unlabeled dataset\n",
      "length of unlabelled dataset: 24300730469]]\n",
      "Totals Probability of the budget: tensor(0.2006, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "#### Selection Complete, Now re-training with augmented subset ####\n",
      "After augmentation, size of train_set:  8150  unlabeled set:  24250  val set:  2000\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.8351658582687378 0.6883435582822086 2.3497955799102783 0.3555 2.3139495849609375 0.3611 308.12210726737976\n",
      "Gain in accuracy:  3.9399999999999977\n",
      "**** Final Metrics after Targeted Learning ****\n",
      "|   Class |   Val Accuracy |   Test Accuracy |\n",
      "|---------+----------------+-----------------|\n",
      "|       0 |           48   |            49.1 |\n",
      "|       1 |            1   |             0.6 |\n",
      "|       2 |           31   |            39.8 |\n",
      "|       3 |           32.5 |            28.8 |\n",
      "|       4 |           42.5 |            38.8 |\n",
      "|       5 |           31   |            30.2 |\n",
      "|       6 |           53   |            51.5 |\n",
      "|       7 |           45   |            49.2 |\n",
      "|       8 |            4   |             2.9 |\n",
      "|       9 |           67.5 |            70.2 |\n",
      "Gain in overall test accuracy:  3.9399999999999977\n",
      "Gain in targeted test accuracy:  1.75\n",
      "Total gain in accuracy:  3.9399999999999977\n",
      "Budget  50  Strategy  WASSAL_R  Method  WASSAL_R\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  8100 Val size:  2000 Lake size:  24300\n",
      "Indices of randomly selected classes for imbalance:  [8 1]\n",
      "Saving results to:  /home/wassal/trust-wassal/tutorials/results/cifar10/classimb/WASSAL_R/50/exp1\n",
      "Total samples from imbalanced classes as Queries (Size of query set):  500\n",
      "Total samples from imbalanced classes as Private (Size of private set):  9600\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WASSAL_R' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m strategy, method \u001b[39min\u001b[39;00m strategies:\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBudget \u001b[39m\u001b[39m\"\u001b[39m,b,\u001b[39m\"\u001b[39m\u001b[39m Strategy \u001b[39m\u001b[39m\"\u001b[39m,strategy,\u001b[39m\"\u001b[39m\u001b[39m Method \u001b[39m\u001b[39m\"\u001b[39m,method)\n\u001b[0;32m---> 19\u001b[0m     run_targeted_selection(data_name, \n\u001b[1;32m     20\u001b[0m                             datadir, \n\u001b[1;32m     21\u001b[0m                             feature, \n\u001b[1;32m     22\u001b[0m                             model_name, \n\u001b[1;32m     23\u001b[0m                             b,             \u001b[39m# updated budget\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m                             split_cfg, \n\u001b[1;32m     25\u001b[0m                             learning_rate, \n\u001b[1;32m     26\u001b[0m                             run, \n\u001b[1;32m     27\u001b[0m                             device, \n\u001b[1;32m     28\u001b[0m                             computeClassErrorLog,\n\u001b[1;32m     29\u001b[0m                             strategy, \n\u001b[1;32m     30\u001b[0m                             method)\n",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m, in \u001b[0;36mrun_targeted_selection\u001b[0;34m(dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m     87\u001b[0m     for_query_set \u001b[39m=\u001b[39m getQuerySet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n\u001b[1;32m     88\u001b[0m     for_private_set \u001b[39m=\u001b[39m getPrivateSet(ConcatWithTargets(train_set,val_set),sel_cls_idx)\n\u001b[0;32m---> 89\u001b[0m     strategy_sel \u001b[39m=\u001b[39m WASSAL_R(train_set, unlabeled_lake_set,for_query_set, for_private_set,model,num_cls,strategy_args)\n\u001b[1;32m     92\u001b[0m \u001b[39m# Loss Functions\u001b[39;00m\n\u001b[1;32m     93\u001b[0m criterion, criterion_nored \u001b[39m=\u001b[39m loss_function()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WASSAL_R' is not defined"
     ]
    }
   ],
   "source": [
    "# List of strategies\n",
    "strategies = [\n",
    "    (\"WASSAL\", \"WASSAL\"),\n",
    "    (\"WASSAL_P\", \"WASSAL_P\"),\n",
    "    (\"SIM\", 'fl1mi'),\n",
    "    (\"SIM\", 'fl2mi'),\n",
    "    (\"SIM\", 'gcmi'),\n",
    "    (\"SIM\", 'logdetmi'),\n",
    "    (\"random\", 'random'),\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# Loop for each budget from 50 to 400 in intervals of 50\n",
    "for b in range(50, 401, 50):\n",
    "    # Loop through each strategy\n",
    "    for strategy, method in strategies:\n",
    "        print(\"Budget \",b,\" Strategy \",strategy,\" Method \",method)\n",
    "        run_targeted_selection(data_name, \n",
    "                                datadir, \n",
    "                                feature, \n",
    "                                model_name, \n",
    "                                b,             # updated budget\n",
    "                                split_cfg, \n",
    "                                learning_rate, \n",
    "                                run, \n",
    "                                device, \n",
    "                                computeClassErrorLog,\n",
    "                                strategy, \n",
    "                                method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfc956",
   "metadata": {},
   "source": [
    "# WASSAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f962e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name,\n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg,\n",
    "               learning_rate,\n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"WASSAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240bc93",
   "metadata": {},
   "source": [
    "# Submodular Mutual Information (SMI)\n",
    "\n",
    "We let $V$ denote the ground-set of $n$ data points $V = \\{1, 2, 3,...,n \\}$ and a set function $f:\n",
    " 2^{V} \\xrightarrow{} \\Re$. Given a set of items $A, B \\subseteq V$, the submodular mutual information (MI)[1,3] is defined as $I_f(A; B) = f(A) + f(B) - f(A \\cup B)$. Intuitively, this measures the similarity between $B$ and $A$ and we refer to $B$ as the query set.\n",
    "\n",
    "In [2], they extend MI to handle the case when the target can come from an auxiliary set $V^{\\prime}$ different from the ground set $V$. For targeted data subset selection, $V$ is the source set of data instances and the target is a subset of data points (validation set or the specific set of examples of interest).\n",
    "Let $\\Omega  = V \\cup V^{\\prime}$. We define a set function $f: 2^{\\Omega} \\rightarrow \\Re$. Although $f$ is defined on $\\Omega$, the discrete optimization problem will only be defined on subsets $A \\subseteq V$. To find an optimal subset given a query set $Q \\subseteq V^{\\prime}$, we can define $g_{Q}(A) = I_f(A; Q)$, $A \\subseteq V$ and maximize the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3045e5",
   "metadata": {
    "id": "PeujlNrf8lLa"
   },
   "source": [
    "# FL1MI\n",
    "\n",
    "In the first variant of FL, we set the unlabeled dataset to be $V$. The SMI instantiation of FL1MI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\sum_{i \\in V}\\min(\\max_{j \\in A}s_{ij}, \\eta \\max_{j \\in Q}sq_{ij})\n",
    "\\end{align}\n",
    "\n",
    "The first term in the min(.) of FL1MI models diversity, and the second term models query relevance. An increase in the value of $\\eta$ causes the resulting summary to become more relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96a686",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "d978b42c8c2d4ce69ca7a3233094ba73",
      "14a33535616d4fa993d20e525666af04",
      "bb6c0909711948b98f0a0b767cb9201e",
      "6792820b58df4d2b86386c29aaad1a9f",
      "9d386f74f6c4469bbbf1ac6b2a49afed",
      "4ee6a0573c6845458996a59679ac6068",
      "0074c4d4c28d4dad87a3eaaac8458479",
      "cbe081bb90d04faaae98fef1c9921180"
     ]
    },
    "id": "QHn0QzY78lLa",
    "outputId": "a203ea99-fb5b-4bfa-99e5-699ef0715d16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fc84e",
   "metadata": {
    "id": "kp9_ZU7I8lLa"
   },
   "source": [
    "# FL2MI\n",
    "\n",
    "In the V2 variant, we set $D$ to be $V \\cup Q$. The SMI instantiation of FL2MI can be defined as:\n",
    "\\begin{align} \\label{eq:FL2MI}\n",
    "I_f(A;Q)=\\sum_{i \\in Q} \\max_{j \\in A} sq_{ij} + \\eta\\sum_{i \\in A} \\max_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "FL2MI is very intuitive for query relevance as well. It measures the representation of data points that are the most relevant to the query set and vice versa. It can also be thought of as a bidirectional representation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e29177",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZKsblhs8lLa",
    "outputId": "5983fdfb-6541-43e9-a0f0-304ddfa543b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog, \n",
    "               \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ea94d",
   "metadata": {
    "id": "9f6qyrpA8lLc"
   },
   "source": [
    "# GCMI\n",
    "\n",
    "The SMI instantiation of graph-cut (GCMI) is defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=2\\sum_{i \\in A} \\sum_{j \\in Q} sq_{ij}\n",
    "\\end{align}\n",
    "Since maximizing GCMI maximizes the joint pairwise sum with the query set, it will lead to a subset similar to the query set $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6e91d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MM9copHy8lLc",
    "outputId": "1c5e4fe8-2f3c-4137-9f64-c728e00c34d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07323a40",
   "metadata": {
    "id": "mGDPAjDR8lLc"
   },
   "source": [
    "# LOGDETMI\n",
    "\n",
    "The SMI instantiation of LogDetMI can be defined as:\n",
    "\\begin{align}\n",
    "I_f(A;Q)=\\log\\det(S_{A}) -\\log\\det(S_{A} - \\eta^2 S_{A,Q}S_{Q}^{-1}S_{A,Q}^T)\n",
    "\\end{align}\n",
    "$S_{A, B}$ denotes the cross-similarity matrix between the items in sets $A$ and $B$. The similarity matrix in constructed in such a way that the cross-similarity between $A$ and $Q$ is multiplied by $\\eta$ to control the trade-off between query-relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad455b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1QbcrJk8lLd",
    "outputId": "1a7604b8-dee7-468f-d9bf-7e8b202c18d9"
   },
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec7d2d",
   "metadata": {
    "id": "3dZqCZ1v8lLe"
   },
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389578a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlbOpp438lLe",
    "outputId": "f754727a-dfb5-4853-d4c1-8a96115182db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fddb5",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc215ed",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e524cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_targeted_selection(data_name, \n",
    "               datadir, \n",
    "               feature, \n",
    "               model_name, \n",
    "               budget, \n",
    "               split_cfg, \n",
    "               learning_rate, \n",
    "               run, \n",
    "               device, \n",
    "               computeClassErrorLog,\n",
    "               \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf8b9c",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asnani. Submodular combinatorialinformation measures with applications in machine learning.arXiv preprint arXiv:2006.15412,2020\n",
    "\n",
    "\n",
    "[2] Kaushal V, Kothawade S, Ramakrishnan G, Bilmes J, Iyer R. PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. arXiv preprint arXiv:2103.00128. 2021 Feb 27.\n",
    "\n",
    "\n",
    "[3] Anupam Gupta and Roie Levin. The online submodular cover problem. InACM-SIAM Symposiumon Discrete Algorithms, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431a829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "submodlib_cifar_classimb.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0074c4d4c28d4dad87a3eaaac8458479": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a33535616d4fa993d20e525666af04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee6a0573c6845458996a59679ac6068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6792820b58df4d2b86386c29aaad1a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe081bb90d04faaae98fef1c9921180",
      "placeholder": "​",
      "style": "IPY_MODEL_0074c4d4c28d4dad87a3eaaac8458479",
      "value": " 170499072/? [00:07&lt;00:00, 23215900.76it/s]"
     }
    },
    "9d386f74f6c4469bbbf1ac6b2a49afed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bb6c0909711948b98f0a0b767cb9201e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee6a0573c6845458996a59679ac6068",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d386f74f6c4469bbbf1ac6b2a49afed",
      "value": 170498071
     }
    },
    "cbe081bb90d04faaae98fef1c9921180": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d978b42c8c2d4ce69ca7a3233094ba73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb6c0909711948b98f0a0b767cb9201e",
       "IPY_MODEL_6792820b58df4d2b86386c29aaad1a9f"
      ],
      "layout": "IPY_MODEL_14a33535616d4fa993d20e525666af04"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
